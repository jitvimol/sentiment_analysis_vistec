{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN THIS NOTEBOOK.   SIMPLE SAVE FOR LEARNING PURPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another LDA (Thai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datadairy.blogspot.com/2019/06/lda-topic-modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import import_ipynb\n",
    "#import utilities.config as config\n",
    "import datetime\n",
    "import calendar\n",
    "import scipy.stats\n",
    "from datetime import timedelta, date\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count #For Parallel\n",
    "from IPython.display import clear_output\n",
    "\n",
    "cores = cpu_count() #Number of CPU cores on your system\n",
    "partitions = cores #Define as many partitions as you want to run parallel\n",
    "\n",
    "stopwords = list(thai_stopwords()) \n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = multiprocessing.Pool(cores)     #สร้าง multiprocessor จำนวนเท่ากับ core cpu ของ server\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "#ตัดคำ\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    global progress\n",
    "    progress = progress + 1\n",
    "    if(progress % 250 == 0):\n",
    "        clear_output()\n",
    "        printProgressBar (progress, allLen, 0,0, '','', 1,100, '█')\n",
    "        \n",
    "    for token in word_tokenize(text,engine='newmm'): \n",
    "        token = token.strip()\n",
    "#         if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >5 :\n",
    "#             result.append(lemmatize_stemming(token))\n",
    "        \n",
    "        mystopword = ['ค่ะ','ครับ','จะ','สวัสดี','พี่','ผม','สอบถาม',' ','','ไม่','ได้','ไม่ได้','ที่','อยาก','ขอ', 'มี', 'แล้ว', 'คะ','มัน', 'ใช้', 'ว่า','บี','ให้','ไป','มา','อ่ะ','แต่','พอดี', 'ทราบ','นี้' ,'ต้อง', 'ของ','เลย','หน่อย','เรื่อง','เข้า','เป็น','ต้องการ','เรา','อะไร','ทำไม','คือ','การ','อยู่']\n",
    "        if token not in mystopword:\n",
    "            result.append(lemmatize_stemming(token)) \n",
    "\n",
    "#         stopwords.append(\"สวัสดี\")\n",
    "#         if token not in stopwords and len(token) > 4 :\n",
    "#             result.append(lemmatize_stemming(token))  \n",
    "\n",
    "#         result.append(lemmatize_stemming(token))  \n",
    "            \n",
    "#     for token in word_tokenize(text,engine='newmm'): \n",
    "#         token = token.strip()\n",
    "#     stopwords = list(thai_stopwords())    \n",
    "#     result = [i for i in text if i not in stopwords]\n",
    "    return result\n",
    "\n",
    "def func(documents):\n",
    "    res = documents['headline_text'].map(preprocess)\n",
    "    return res\n",
    "\n",
    "def cleanText(documents):\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('|',' ')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('\\\\',' ')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('\\/',' ')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('.',' ')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('_','')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('\\d+', '')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('-',' ')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('+',' ')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('+',' ')\n",
    "    pattern = '|'.join(['&', '%', ';', '='])\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace(pattern, ' ')\n",
    "    documents = documents.loc[~documents['headline_text'].str.contains(\"img\")]\n",
    "    documents.headline_text.replace({r\"[a-zA-Z]+\":''}, regex=True, inplace=True)\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace(':',' ')\n",
    "    documents['headline_text'] = documents['headline_text'].str.replace('#',' ')\n",
    "    documents['headline_text'] = documents['headline_text'].str.strip()\n",
    "    documents = documents.loc[documents['headline_text']!= \"\"]\n",
    "    documents = documents.dropna().reset_index(drop=True)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = train.copy()\n",
    "documents = documents.rename(columns={'texts':'headline_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = parallelize(documents,cleanText)\n",
    "documents.head()\n",
    "documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "import nltk\n",
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLen = documents.shape[0]\n",
    "progress = 0\n",
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total,mode = 0,current_percent = 0, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    if mode == 0 :\n",
    "        filledLength = int(length * iteration // total)\n",
    "        bar = fill * filledLength + '-' * (length - filledLength)\n",
    "        print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "        # Print New Line on Complete\n",
    "        if iteration == total:\n",
    "            print()\n",
    "    elif mode == 1:\n",
    "        if current_percent != round(float(percent),1):\n",
    "            current_percent = round(float(percent),1)\n",
    "            print(percent+\"%\")\n",
    "        return current_percent\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "processed_docs = parallelize(documents,func)# preprocess, ['headline_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.sample(10)\n",
    "\n",
    "# x=890     \n",
    "# documents.iloc[x]['headline_text']\n",
    "# processed_docs.iloc[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ตัดคำฟุ่มเฟือยออก โดยลบคำที่ปรากฏบ่อยเกิน 50% และให้เหลือไปถึง1พันคำ\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=1000)  #reduce to 1K word\n",
    "#len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim doc2bow\n",
    "# For each document we create a dictionary reporting how many\n",
    "# words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n",
    "\n",
    "# run LDA แบ่งคำออกเป็น9กลุ่ม โดยใช้ cpu ทุกตัว (serverที่เราใช้มี32cpu)\n",
    "#worker = cpu core amount -1\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=7, id2word=dictionary, passes=2, workers=31)\n",
    "\n",
    "# For each topic, we will explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['billing and payment', 'promotions', 'other queries', 'internet', 'international dialing', 'true money', 'lost and stolen']\n",
    "\n",
    "topic 0 = \n",
    "topic 1 =\n",
    "topic 2 = \n",
    "topic 3 =\n",
    "topic 4 = \n",
    "topic 5 =\n",
    "topic 6 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=processed_docs):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        row\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,0), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=processed_docs.values.tolist())\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[0]['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic1 = df_dominant_topic[['Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords','Text']]\n",
    "df_dominant_topic1 = pd.concat([df_dominant_topic1,train2['destination']],axis=1)\n",
    "df_dominant_topic1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category = train['destination'].unique().tolist()\n",
    "# category1 = pd.Series(category)\n",
    "# for x in range(0,len(category1)):\n",
    "#     df_dominant_topic1.loc[df_dominant_topic1['Dominant_Topic'] == x , 'predict' ]= category1[x]\n",
    "\n",
    "    \n",
    "# train2['hit'] = (train2['destination'] == train2['predict'])\n",
    "# train2['hit'].value_counts()\n",
    "# train2.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find category topic based on category value\n",
    "\n",
    "category_LDA = [ 'lost and stolen', 'international dialing','other queries', 'billing and payment', 'internet' , 'true money', 'promotions']\n",
    "\n",
    "for x in range(0,7):\n",
    "    print(\"category : \", x,\" | \",category_LDA[x] )\n",
    "    df_dominant_topic1[df_dominant_topic1['Dominant_Topic'] == x]['destination'].value_counts().nlargest(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toplic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word list to sentence\n",
    "train_clean1 = train_clean.copy()\n",
    "train_clean1['texts'] = train_clean['texts'].map(lambda x: ' '.join(x))\n",
    "train_clean1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_clean1['texts'].tolist()\n",
    "topics = train_clean1['destination'].tolist()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix = CountVectorizer(max_features=1000)\n",
    "vectors = matrix.fit_transform(texts).toarray()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "vectors_train, vectors_test, topics_train, topics_test = train_test_split(vectors, topics, test_size=0.15, random_state=260)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(vectors_train, topics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the testing set\n",
    "topics_pred = classifier.predict(vectors_test)\n",
    "\n",
    "# ...and measure the accuracy of the results\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(topics_test, topics_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo Topic Classification but do adding function remove thaistopword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customer-made stop word\n",
    "train_clean_temp = train_clean.copy()\n",
    "\n",
    "for index in range(len(train_clean)):\n",
    "    mystopword = ['ค่ะ','ครับ','จะ','สวัสดี','พี่','ผม','สอบถาม',' ','','ไม่','ได้','ไม่ได้','ที่','อยาก','ขอ', 'มี', 'แล้ว', 'คะ','มัน', 'ใช้', 'ว่า','บี','ให้','ไป','มา','อ่ะ','แต่','พอดี', 'ทราบ','นี้' ,'ต้อง', 'ของ','เลย','หน่อย','เรื่อง','เข้า','เป็น','ต้องการ','เรา','อะไร','ทำไม','คือ','การ','อยู่']\n",
    "    train_clean_temp.iloc[index]['texts'] = [i for i in train_clean.iloc[index]['texts'] if i not in mystopword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word list to sentence\n",
    "train_clean1['texts'] = train_clean_temp['texts'].map(lambda x: ' '.join(x))\n",
    "train_clean1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_clean1['texts'].tolist()\n",
    "topics = train_clean1['destination'].tolist()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix = CountVectorizer(max_features=1000)\n",
    "vectors = matrix.fit_transform(texts).toarray()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "vectors_train, vectors_test, topics_train, topics_test = train_test_split(vectors, topics, test_size=0.15, random_state=260)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(vectors_train, topics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the testing set\n",
    "topics_pred = classifier.predict(vectors_test)\n",
    "\n",
    "# ...and measure the accuracy of the results\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(topics_test, topics_pred))\n",
    "\n",
    "print(color.BOLD + \"=========Result is worser==========\" + color.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Latent Dirichlet Allocation and Non-Negative Matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/python-for-nlp-topic-modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train-test\n",
    "train_clean_LDA = train_clean1.copy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(train_clean_LDA, test_size=0.15, random_state=260)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=None, lowercase=None, ngram_range=(1,2), min_df=20, sublinear_tf=True)\n",
    "train_clean1['texts'] = train_clean1['texts'].astype(str)\n",
    "\n",
    "tfidf_fit  = tfidf.fit(train_clean1['texts'])\n",
    "text_train = tfidf_fit.transform(train_df['texts'])\n",
    "text_valid = tfidf_fit.transform(valid_df['texts'])\n",
    "text_test  = tfidf_fit.transform(test['texts'])\n",
    "#text_train.shape, text_valid.shape\n",
    "\n",
    "X_train = text_train.toarray()\n",
    "X_valid = text_valid.toarray()\n",
    "X_test = text_test.toarray()\n",
    "\n",
    "y_train = train_df['destination']\n",
    "y_valid = valid_df['destination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=7, random_state=42)\n",
    "LDA.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_values = LDA.transform(X_train)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 Follow https://colab.research.google.com/drive/1uaOsotWWgHGGceXkoE75xzSdlYsMZ4RK?authuser=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do tokenization\n",
    "train1 = train.copy()\n",
    "train1['texts'] = train1['texts'].apply(lambda x : my_split_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = train1['texts'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words + count word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tokens_list_j = [','.join(tkn) for tkn in tokens_list]\n",
    "cvec = CountVectorizer(analyzer=lambda x:x.split(','))\n",
    "c_feat = cvec.fit_transform(tokens_list_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_feat[:,:20].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(analyzer=lambda x:x.split(','),)\n",
    "t_feat = tvec.fit_transform(tokens_list_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_feat[:,:5].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tvec.idf_),len(tvec.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_feat[:,:5].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 : use pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train1[\"texts\"], train1['destination'], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    (\"vect\", CountVectorizer(tokenizer=None, ngram_range=(1,2) , lowercase = False)),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", MultinomialNB(alpha=0.001)),\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "text_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df (english) for testing\n",
    "\n",
    "# raw_data = {'texts':['Whether you’re new to spaCy, or just want to brush up on some NLP basics and implementation details ', 'This page should have you covered. ', 'Each section will explain one of spaCy’s features in simple terms and with examples or illustrations. ', 'Some sections will also reappear across the usage guides as a quick introduction.'], 'Age':[20, 21, 19, 18]} \n",
    "# train = pd.DataFrame(raw_data)\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unused as we manually do text tokenization\n",
    "\n",
    "# Filter to get only text that has len > 4\n",
    "\n",
    "# train = train[train['texts'].map(len) > 4]\n",
    "# len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('th')\n",
    "my_nlp_experiment = nlp.setup(data=train, target='texts',custom_stopwords=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = nlp.create_model(model=hdp', multi_core=True, num_topics=2)\n",
    "display(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_result = nlp.assign_model(lda_model)\n",
    "display(lda_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "170px",
    "width": "195px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
